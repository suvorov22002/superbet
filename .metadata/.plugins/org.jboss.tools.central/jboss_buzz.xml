<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Profiling and optimizing executable model generation</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/tRkd6eZYwzo/profiling-and-optimizing-executable-model-generation.html" /><author><name>Mario Fusco</name></author><id>https://blog.kie.org/2021/06/profiling-and-optimizing-executable-model-generation.html</id><updated>2021-06-10T07:40:41Z</updated><content type="html">INTRODUCING DROOLS EXECUTABLE MODEL During the development of Drools 7.x we with the goal of providing a pure Java-based representation of a rule set. One of the main drives for its introduction was to speed up the creation of a KieContainer starting from a compiled kjar. In fact what Drools 6.x did during the kjar compilation and packaging was simply to parse and verify the drl files and other Drools related assets. After this validation it put those files unchanged into the kjar. This implies that when the kjar was installed into a KieContainer, Drools had to reparse those files from scratch, resulting in quite a long time required to create a KieContainer, especially for large rule bases. What the executable model does to solve this problem is replacing, at kjar creation time, the rules written in drl like for instance the following  rule "Sum ages of Persons starting with M" when accumulate ( $p: Person ( name.startsWith(\"M\") ); $sum : sum($p.getAge()) ) then insert(new Result($sum)); end with a purely Java-based representation of the same rule like Variable var_$p = D.declarationOf(Person.class, "$p"); Variable var_age = D.declarationOf(Integer.class); Variable var_$sum = D.declarationOf(Integer.class, "$sum"); Rule rule = D.rule("Sum ages of Persons starting with M") .build( D.accumulate(D.pattern(var_$p) .bind(var_age, (Person this) -&gt; this.getAge()) .expr((Person _this) -&gt; _this.getName().startsWith("M"), D.reactOn("name")), D.accFunction(IntegerSumAccumulateFunction::new, var_age).as(var$sum)), D.on(var_$sum).execute((Drools drools, Integer $sum) -&gt; { drools.insert(new Result($sum)); })); This Java representation, invoking an ad-hoc created DSL, is (in its current state but we also planned to work on this) probably too complex and too low level. It would be very hard for a Drools user without a deep understanding of the engine internals to write it correctly. However it can be automatically generated like the kie-maven-plugin does. In this way in Drools 7.x, when the kjar is installed into a KieContainer, executing the DSL to interpret it is enough to instantiate the phreak network. That’s the meaning of the term "executable model". EXTERNALIZING EXECUTABLE MODEL’S LAMBDA EXPRESSIONS Using this executable model not only dramatically reduced (even more than 90%) the time required to create a KieContainer, but also allowed runtime improvements. In fact the predicates in the executable model are lambdas and are used to evaluate conditions inside the phreak network nodes in a much faster and cleaner way than interpreting those constraints via MVEL (and eventually transforming it into bytecode via ASM after a few iterations) as it did before. Unfortunately we also found that this strategy had a major drawback for what regards memory occupation. In fact the lambdas are referred inside the phreak nodes but in turn they keep a reference to all the intermediate data structures used to interpret the model and to create the phreak network. In essence those data structures are no longer useful once the phreak network has been instantiated, but the lambda expressions keep them alive and then the garbage collector cannot reclaim the memory space that they occupy. We addressed this issue adding a second stage in the executable model generation process that rewrites all those lambdas transforming them into external classes. After this rework the generated executable model for the former rules now looks like this Rule rule = D.rule("Sum ages of Persons starting with M") .build( D.accumulate(D.pattern(var_$p) .bind(var_age, LambdaExtractor1.INSTANCE) .expr(LambdaPredicate2.INSTANCE, D.reactOn("name")), D.accFunction(IntegerSumAccumulateFunction::new, var_age) .as(var_$sum)), D.on(var_$sum).execute(LambdaConsequence3.INSTANCE)); @MaterializedLambda public enum LambdaExtractor1 implements Function1 { INSTANCE; @Override public Integer apply(Person _this) { return _this.getAge(); } } @MaterializedLambda public enum LambdaPredicate2 implements Predicate1 { INSTANCE; @Override public boolean test(Person _this) { return _this.getName().startsWith("M"); } } @MaterializedLambda public enum LambdaConsequence3 implements Block2 { INSTANCE; @Override public void execute(Drools drools, Integer $sum) { drools.insert(new Result($sum)); } } In this way the lambda expressions no longer hold a reference to the rest of the executable model that now can be successfully garbage collected once its duty is terminated. IMPROVING COMPILE TIME PERFORMANCES To recap, the introduction of the executable model allowed both a very fast KieContainer creation and some improvements and simplification of the runtime with the possibility of getting rid of mvel and ASM.  The last drawback of this is in the fact that we considerably increased the time required by the kie-maven-plugin to produce a kjar out of a rule base: translating each rule and generating the corresponding executable model Java source code is a time consuming task and the lambda externalization post-processing phase made this even longer. In order to at least mitigate this problem and also speed up the generation of the executable model we decided to take two complementary approaches. First we figured out that the whole executable model generation process was unnecessarily sequential and that we could better leverage our shiny multicore CPUs parallelizing it in 2 different points. Since version 7.55 instead of translating each rule into the corresponding executable model one after the other we do this in parallel performing this translation inside the Fork/Join thread pool of a parallel Stream. In a very similar way we have been also able to parallelize the externalization of the lambda expressions with the only precaution of avoiding the concurrent modification of the AST generated by , the tool that we used to programmatically generate the source code of the executable model. Second, we also tried to figure out if there was the possibility of improving the performances of our executable model generator regardless of its internal parallelization. Trying to guess where a Java program consumes the biggest part of time and resources is nearly impossible. The only way to do this correctly is measuring and using the right tools. First of all we created a that compiles a real-world project containing more than 2,000 rules provided by a customer, who has been also one of the early adopters of the executable model, and obfuscated accordingly. We used it both to profile the executable model generator and of course to check how effective our improvements are. To profile our benchmark we used that has the advantages of being seamlessly integrated with JMH, having an extremely low overhead and producing effective and easy to read . To make async-profiler generating the flamegraphs profiling the benchmark under execution is sufficient to add to the command line used to launch JMH a -prof argument like the following: -prof "async:output=flamegraph;event=cpu;simple=true;dir=/tmp/Profile;width=1920;libPath=/home/mario/software/async-profiler-1.8.1-linux-x64/build/libasyncProfiler.so" where * async is to use async-profiler as profiling tool. There are other profilers available like . * event is the feature under profiling: cpu (the default) is to find the most time-consuming method. You can also use alloc to check the memory allocation, cache-misses and others. * output is indeed the format used to produce the profiling results. The supported ones are text, collapsed, flamegraph, tree, jfr (default is text) but in general I find flamegraphs very visual and then easy to read. * simple=true is to print only the simple class names instead of the fully qualified ones. Having the complete package names in the flamegraph make it much harder to read * dir is the directory where the output will be written * width is the width in pixels of the flamegraph, the bigger the better &#x1f642; * libPath is the location where you have downloaded the async-profiler library PROFILING THE EXECUTABLE MODEL GENERATION With this setup, a first run to our benchmark revealed a quite unexpected but very evident issue The colors in this flamegraph have this meaning:  * green are Java methods * yellow are C++ methods * red are user-mode native calls * orange are kernel calls In our case it was clear that we were consuming the biggest part of the time collecting garbage instead of doing any useful work. In other words our main problem was that we were allocating far too many objects but still we had no clue of what was the root cause of this. To discover this it was necessary to rerun the benchmark, but this time profiling the memory allocation using the argument event=alloc. Note that to profile the memory allocation with async-profiler you need to run it on a JVM compiled with HotSpot debug symbols. By doing so we obtained the following flamegraph. Reading and interpreting this graph is not trivial because the objects’ allocation is of course scattered on almost every method execution. For each profiling session however, asnyc-profiler also generates a second flamegraph showing the stack in reversed order, that in situations like this can be of more immediate readability. Here the main culprit of the excessive memory allocation is much more visible: JavaParser is creating a lot of Streams to repeatedly parse very small portions of Java code. It tuned out that we are misusing JavaParser forcing it to perform thousands of microparsing when adding an import or an annotation to an AST or simply to convert a String representing a Class name into the corresponding JavaParser Type in order to use it with the rest of JavaParser API. For instance to add an import to a JavaParser CompilationUnit we did something like: cu.addImport(“java.util.List”); This caused JavaParser, something that in all honesty isn’t immediately evident from its documentation, to trigger a parsing of the String “java.util.List” in order to internally create that portion of AST representing it. This can be easily avoided because we already know in advance how that AST should be made. Replacing the former statement with one that programmatically creates the AST portion for the import to be added like the following: cu.addImport(new ImportDeclaration(new Name(“java.util.List”), false, false)); made it possible to avoid all those unnecessary and memory consuming parsing operations. The combination of the parallelization of the executable model generation, of the savvier usage of JavaParser API and other micro optimizations found during our profiling session allowed us to bring the time required to generate the executable model for the project under benchmarking from the original 133 seconds Benchmark                             Mode  Cnt    Score    Error  Units KjarCompiler.buildKjarFromBigProject    ss    5  133.018 ± 15.665   s/op to the current 32. Benchmark                             Mode  Cnt    Score    Error  Units KjarCompiler.buildKjarFromBigProject    ss    5   32.823 ±  7.883   s/op We are willing to continue this profiling effort and keep improving the performances of Drools in this and other areas in the near future. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/tRkd6eZYwzo" height="1" width="1" alt=""/&gt;</content><dc:creator>Mario Fusco</dc:creator><feedburner:origLink>https://blog.kie.org/2021/06/profiling-and-optimizing-executable-model-generation.html</feedburner:origLink></entry><entry><title>Connect Quarkus applications with Drogue IoT and LoRaWAN</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/taZaScrSPfQ/connect-quarkus-applications-drogue-iot-and-lorawan" /><author><name>Jens Reimann</name></author><id>9b7c099b-46d7-4173-8eb5-7438420b1c94</id><updated>2021-06-10T07:00:00Z</updated><published>2021-06-10T07:00:00Z</published><summary type="html">&lt;p&gt;The goal of the &lt;a href="https://drogue.io" target="_blank"&gt;Drogue IoT&lt;/a&gt; project is to make it easy to connect devices and cloud-based applications with each other. In this article, we'll show how to implement firmware based on Drogue Device that can communicate with &lt;a href="https://developers.redhat.com/topics/quarkus/" target="_blank"&gt;Quarkus&lt;/a&gt; applications in the cloud using the low-power &lt;a href="https://lora-alliance.org/" target="_blank"&gt;LoRaWAN&lt;/a&gt; protocol.&lt;/p&gt; &lt;h2&gt;Drogue IoT&lt;/h2&gt; &lt;p&gt;A lot of &lt;a href="https://developers.redhat.com/topics/open-source/"&gt;open source&lt;/a&gt; technology already exists in the realm of messaging and IoT (Internet of Things), so it makes sense to try to use as many existing tools as possible. However, technologies change over time, and not everything that exists now is fit for the world of tomorrow. &lt;a href="https://developers.redhat.com/topics/c/"&gt;C and C++&lt;/a&gt; still have issues with memory safety, and &lt;em&gt;cloud native&lt;/em&gt;, &lt;em&gt;&lt;a href="https://developers.redhat.com/topics/serverless-architecture/"&gt;serverless&lt;/a&gt;&lt;/em&gt;, and &lt;em&gt;pods&lt;/em&gt; are concepts that might need a different approach when designing cloud-side applications. So, Drogue IoT is here to help.&lt;/p&gt; &lt;p&gt;Drogue Device is a firmware framework written in &lt;a href="https://www.rust-lang.org/" target="_blank"&gt;Rust&lt;/a&gt; with an actor-based programming model. Drogue Cloud is a thin layer of services that creates an IoT-friendly API for existing technologies like &lt;a href="https://knative.dev/" target="_blank"&gt;Knative&lt;/a&gt; and &lt;a href="https://kafka.apache.org/" target="_blank"&gt;Apache Kafka&lt;/a&gt; and provides a cloud-friendly API using &lt;a href="https://cloudevents.io/" target="_blank"&gt;CloudEvents&lt;/a&gt; on the other side. The idea is not to provide separated components but to give you an overall solution that's ready to run: IoT as a service. The Drogue IoT architecture is shown in Figure 1.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/05/architecture.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/05/architecture.png?itok=-2KxWBEP" width="600" height="200" alt="Overview diagram of the Drogue IoT architecture" title="Drogue IoT architecture overview" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Overview diagram of the Drogue IoT architecture.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;LoRaWAN network coverage&lt;/h2&gt; &lt;p&gt;LoRaWAN is a low-power wireless network that enables you to run a device on batteries for months, sending telemetry data to the cloud every now and then. To accomplish that, you need LoRaWAN network coverage, and &lt;a href="https://www.thethingsnetwork.org/" target="_blank"&gt;The Things Network&lt;/a&gt; (TTN) provides exactly this. You can extend the TTN network by running your own gateway, should your local area lack coverage. TTN provides a public service that allows you to exchange data between devices and applications.&lt;/p&gt; &lt;h2&gt;Drogue Device&lt;/h2&gt; &lt;p&gt;Exchanging data with Drogue Device is easy. The following snippet focuses on the actual code exchanging data:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;let mut tx = String::&lt;heapless::consts::U32&gt;::new(); let led = match self.config.user_led.state().unwrap_or_default() { true =&gt; "on", false =&gt; "off", }; write!(&amp;mut tx, "ping:{},led:{}", self.counter, led).ok(); let tx = tx.into_bytes(); let mut rx = [0; 64]; let result = cfg .lora .request(LoraCommand::SendRecv(&amp;tx, &amp;mut rx)) .unwrap() .await;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice the &lt;code&gt;await&lt;/code&gt; keyword? Yes, that is indeed asynchronous Rust running on an embedded &lt;a href="https://www.st.com/en/evaluation-tools/b-l072z-lrwan1.html" target="_blank"&gt;STM32 Cortex-M0 device&lt;/a&gt;. Thanks to the embassy framework and the drivers in Drogue Device, asynchronous programming becomes pretty simple. And thanks to Rust, your code is less likely to cause any undefined behavior, like corrupted memory.&lt;/p&gt; &lt;h2&gt;Quarkus&lt;/h2&gt; &lt;p&gt;On the cloud side, we want to have a simple "reconcile loop." The device reports its current state, and we derive the desired state from that. This might result in a command that we want to send back to the device. Again, focusing on the code that is important:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;@Incoming("event-stream") @Outgoing("device-commands") @Broadcast public DeviceCommand process(DeviceEvent event) { var parts = event.getPayload().split(","); // then we check if the payload is, what we expect... if (parts.length != 2 || !parts[0].startsWith("ping:") || !parts[1].startsWith("led:")) { // .. if not, return with no command return null; } // check if the configured response is about the LED, and if it matches ... if (!this.response.startsWith("led:") || parts[1].equals(this.response)) { // ... it is not, or it matches, so we return with no command return null; } var command = new DeviceCommand(); command.setDeviceId(event.getDeviceId()); command.setPayload(this.response.getBytes(StandardCharsets.UTF_8)); return command; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Pretty simple, isn't it? Of course, you could make it more complicated, but we leave that up to you.&lt;/p&gt; &lt;p&gt;The Quarkus application consumes CloudEvents, which provide a standardized representation of events for different messaging technologies like &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Kafka&lt;/a&gt;, HTTP, and MQTT.&lt;/p&gt; &lt;h2&gt;Drogue Cloud&lt;/h2&gt; &lt;p&gt;So far, this is pretty straightforward, focusing on the actual use case. However, we are missing a big chunk in the middle: How do we connect Quarkus with the actual device? Sure, we could recreate all of that ourselves—implementing the TTN API, registering devices, processing events. Alternatively, we could simply use Drogue Cloud and let it do the plumbing for us.&lt;/p&gt; &lt;p&gt;Creating a new application and device is easy using the &lt;code&gt;drg&lt;/code&gt; command-line tool:&lt;/p&gt; &lt;pre&gt; &lt;code class="shell language-bash"&gt;$ drg create application my-app $ drg create device --app my-app my-device&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The device registry in Drogue Cloud not only stores device information but can also reconcile with other services. Adding the following information will make it sync with TTN:&lt;/p&gt; &lt;pre&gt; &lt;code class="shell language-bash"&gt;$ drg create application my-app --spec '{ "ttn": { "api": { "apiKey": "...", "owner": "my-ttn-username", "region": "eu1" } } }' $ drg create --app my-app device my-device --spec '{ "ttn": { "app_eui": "0123456789ABCDEF", "dev_eui": "ABCDEF0123456789", "app_key": "0123456789ABCDEF...", "frequency_plan_id": "...", "lorawan_phy_version": "PHY_V1_0", "lorawan_version": "MAC_V1_0" } }'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This creates a new TTN application, registers the device, sets up a webhook, creates the gateway configuration in Drogue Cloud, and ensures that credentials are present and synchronized.&lt;/p&gt; &lt;h2&gt;Learn more in the LoRaWAN end-to-end workshop&lt;/h2&gt; &lt;p&gt;Did that seem a bit fast? Yes, indeed! This is a lot of information for a single article, so we wanted to focus on the most important parts. We put together everything you need to know in a &lt;a href="https://book.drogue.io/drogue-workshops/ttn-lorawan-quarkus/index.html" target="_blank"&gt;workshop&lt;/a&gt; so you can read through it in more detail and get more background, too. By the end of this workshop, you should end up with a web front-end to control your device, as shown in Figure 2.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/05/quarkus-screen.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/05/quarkus-screen.png?itok=Is9RjzDY" width="600" height="400" alt="Screenshot of the workshop's Quarkus web frontend" title="Quarkus workshop screenshot" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The Quarkus web front-end.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Most importantly, you'll have a solid foundation for creating your own applications on top of Drogue IoT.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/10/connect-quarkus-applications-drogue-iot-and-lorawan" title="Connect Quarkus applications with Drogue IoT and LoRaWAN"&gt;Connect Quarkus applications with Drogue IoT and LoRaWAN&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/taZaScrSPfQ" height="1" width="1" alt=""/&gt;</summary><dc:creator>Jens Reimann</dc:creator><dc:date>2021-06-10T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/10/connect-quarkus-applications-drogue-iot-and-lorawan</feedburner:origLink></entry><entry><title type="html">Red Hat Summit 2021 (Ask the Experts) - An open approach to solution architectures</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/5Vyhy9IGs38/red-hat-summit-2021-ask-the-experts-an-open-approach-to-solution-architectures.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/arMRglaChz8/red-hat-summit-2021-ask-the-experts-an-open-approach-to-solution-architectures.html</id><updated>2021-06-10T05:00:00Z</updated><content type="html">This year the event is a bit different as we bridge the gap from pandemic reality to hopefully a form of normalcy.  As the Red Hat Summit site explains to us, this "...event is expanding to become an all-new, flexible conference series, consisting of a 2‑part immersive virtual experience as well as a global tour of small-scale, in-person* events. This series will create collective opportunities to share experiences, innovations, and insights." Part one was from April 27-28, where you started your Red Hat Summit journey by joining us for this no-cost event, where you got the latest news, asked the experts your technology questions, heard from customers around the globe, and explored how open source is innovating the future of the enterprise. All of the sessions are online and available throughout the year on-demand. Part two is from June 15-16, and you can build on what you learned at April’s event with insights from breakout sessions and technical content geared toward the tracks and topics most relevant to your career. You can also interact live with Red Hat professionals at this no-cost event. More depth at this event with the breakout sessions, ask the experts sessions, and a virtual expo hall. Finally, you noticed the star marking the part three event above? That's because this part of the event we hope allows us to meet in-person, but the ongoing situation means Red Hat will "...continue to monitor the ongoing global health crisis and make adjustments to the Red Hat Summit agenda to help ensure the health, safety, and well-being of everyone within the open source community."  If all goes well, then you can cap your Red Hat Summit experience somewhere from Oct - Nov "...by exploring hands-on activities at in-person events that will be held in several cities. The networking opportunities can help you find the inspiration to discover who you want to be and the tools to do what you want to do." There will be hands-on labs, 1-1 meetings, and training courses available during this final event series. This year I'm involved with some friends and colleagues in a few Ask The Experts sessions where you get a live session on a topic with several experts available to ask anything you like about the topic. I've done in April, and I'm going to be involved in the following session next week: Solution architectures are the detailed and structured descriptions of the features, process, and behavior of a solution. It acts as the base of the solution to define, deliver, manage and operate the development process of the solution. It identifies the alternatives of the solutions and its components. In this session, we'll highlight the process that Red Hat uses to compile and publish solution architectures for various business and technology scenarios that are based on actual use cases pertinent to our global customers and partners. It includes the key capabilities of continuous global collaboration with the engineering teams. Date: Tuesday, Jun 15 Time: 16:30 - 17:00 CEST Speakers:  * Eric D. Schabell, Portfolio Architect Technical Director, Red Hat * Will Nix, Senior Manager, Red Hat * E.G Nadhan, Chief Architect and Strategist, Red Hat Make sure you and plan out your agenda as it's free and full of great open source technologies, customer cases, labs, and more. I hope to see you there and if not, then feel free to drop in at a later date as they will be linking recordings of these sessions back into the agenda for your viewing pleasure.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/5Vyhy9IGs38" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/arMRglaChz8/red-hat-summit-2021-ask-the-experts-an-open-approach-to-solution-architectures.html</feedburner:origLink></entry><entry><title type="html">Narayana 5.12.0.Final released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/z5Z0ECqhVZ8/narayana-5120final-released.html" /><author><name>Michael Musgrove</name></author><id>https://jbossts.blogspot.com/2021/06/narayana-5120final-released.html</id><updated>2021-06-09T16:32:00Z</updated><content type="html">This week we released Narayana 5.12.0.Final. You can download it from our website at https://narayana.io/downloads/index.html This release includes a mix of enhancements (including support for CDI 2.0) and bug fixes, full details can be found in the&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/z5Z0ECqhVZ8" height="1" width="1" alt=""/&gt;</content><dc:creator>Michael Musgrove</dc:creator><feedburner:origLink>https://jbossts.blogspot.com/2021/06/narayana-5120final-released.html</feedburner:origLink></entry><entry><title>How to implement employee rostering with Red Hat Business Optimizer</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/yhdzn3y8ujE/how-implement-employee-rostering-red-hat-business-optimizer" /><author><name>Li Khia Lim</name></author><id>ae828473-8e47-49cf-87dd-1e26b404663a</id><updated>2021-06-09T07:00:00Z</updated><published>2021-06-09T07:00:00Z</published><summary type="html">&lt;p&gt;This article discusses and demonstrates how you can implement &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_decision_manager/7.1/html/running_and_modifying_the_employee_roster_starter_application_for_red_hat_business_optimizer_using_an_ide/optashift-er-overview-con"&gt;duty rostering&lt;/a&gt; with &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_decision_manager/7.0/html/installing_and_configuring_red_hat_business_optimizer/optimizer-about-optimizer-con"&gt;Red Hat Business Optimizer&lt;/a&gt;. Business Optimizer, a component in &lt;a href="https://developers.redhat.com/products/red-hat-decision-manager/overview"&gt;Red Hat Decision Manager&lt;/a&gt;, is an &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;AI&lt;/a&gt; (artificial intelligence) constraint solver that optimizes planning and scheduling problems with Decision Manager.&lt;/p&gt; &lt;p&gt;The duty rostering use case requires us to assign shifts to employees based on their respective duty—for example, cleaning, drying, delivery, and so on. The following hard constraint conditions must be met:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Employees are only assigned one shift each day.&lt;/li&gt; &lt;li&gt;Any shifts that require a specific employee duty are assigned to an employee who can perform that particular duty.&lt;/li&gt; &lt;li&gt;Once an employee schedules a shift off, the shift is reassigned to another employee.&lt;/li&gt; &lt;li&gt;No employee is assigned to a shift that begins less than 10 hours after their previous shift ends.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Additionally, there is a soft constraint that all employees should be assigned to the same number of shifts.&lt;/p&gt; &lt;h2&gt;Planning the employee roster domain model&lt;/h2&gt; &lt;p&gt;Before starting with the duty rostering implementation, let's first plan the resources we need to draw out a domain model, which is shown in Figure 1.&lt;/p&gt; &lt;p&gt;The required resource classes here are &lt;code&gt;Employee&lt;/code&gt;, &lt;code&gt;Duty&lt;/code&gt;, &lt;code&gt;Timeslot&lt;/code&gt;, &lt;code&gt;Shift&lt;/code&gt;, and &lt;code&gt;DayOfRequest&lt;/code&gt;. These classes are &lt;em&gt;problem facts&lt;/em&gt; that do not change during the planning.&lt;/p&gt; &lt;p&gt;The planning result class is &lt;code&gt;ShiftAssignment&lt;/code&gt;. It holds information about an employee and their assigned shift representing a one-to-one relationship. This relationship is referred to as the &lt;em&gt;planning entity&lt;/em&gt; that changes during solving. For example, the shift can be assigned to another employee. &lt;code&gt;Employee&lt;/code&gt; is the &lt;em&gt;planning variable&lt;/em&gt; that changes during planning.&lt;/p&gt; &lt;p&gt;The class that holds everything needed for planning is called the &lt;code&gt;DutyRoster&lt;/code&gt; class. This is the &lt;em&gt;planning solution&lt;/em&gt; that contains the dataset for the solver to solve. The &lt;code&gt;DutyRoster&lt;/code&gt; class contains a list of employees that comprise the &lt;em&gt;planning range&lt;/em&gt;. This is the set of possible planning values for a planning variable.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="The example DutyRoster domain model." data-entity-type="file" data-entity-uuid="6db7c755-1104-4ad4-9bf9-0dba57d48f82" src="https://developers.redhat.com/sites/default/files/inline-images/Diagram1.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1: The example &lt;code&gt;DutyRoster&lt;/code&gt; domain model.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Just to share my thought process, here are the steps I took to create this data model:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;List out all the resources required to come out with the rostering.&lt;/li&gt; &lt;li&gt;Identify the relationship between the resources. Normalize it by eliminating redundancy and inconsistent dependency.&lt;/li&gt; &lt;li&gt;Create the class to hold information (such as shift, timeslot, or employee) about the rostering and what information within this class will change during the solving process. For this use case, it is the employee, as it will change depending on the rules defined.&lt;/li&gt; &lt;li&gt;Create the class to hold all the information (e.g., &lt;code&gt;DutyRoster&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Review the data model structure to see if it matches with the attributes required for rules (in this case, soft constraints and hard constraints). Repeat the process until everything is in place.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Use case implementation with Red Hat Decision Manager&lt;/h2&gt; &lt;p&gt;We will develop our use case using Red Hat Decision Manager 7.7. After installation, log in to Decision Central and perform the following steps:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Create a new project.&lt;/li&gt; &lt;li&gt;Create data objects for &lt;code&gt;Employee&lt;/code&gt;, &lt;code&gt;Duty&lt;/code&gt;, &lt;code&gt;Timeslot&lt;/code&gt;, &lt;code&gt;Shift&lt;/code&gt;, &lt;code&gt;DayOfRequest&lt;/code&gt; as the problem facts, as you see in Figure 2. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Using Decision Central to create a new project and data objects as the problem facts." data-entity-type="file" data-entity-uuid="4400ddf5-9cfc-4612-ae7e-5407fcf05e7d" src="https://developers.redhat.com/sites/default/files/inline-images/Diagram2_0.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2: Using Decision Central to create a new project and data objects as the problem facts.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt;Create the data object for &lt;code&gt;ShiftAssignment&lt;/code&gt; as the planning entity, as shown in Figure 3. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Creating the data object for ShiftAssignment in Decision Central" data-entity-type="file" data-entity-uuid="f92a29eb-7753-4a9d-b2ab-53580215e557" src="https://developers.redhat.com/sites/default/files/inline-images/Diagram3_0.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 3: Create the data object for &lt;code&gt;ShiftAssignment&lt;/code&gt; as the planning entity.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt;Create the data object for &lt;code&gt;DutyRoster&lt;/code&gt; as the planning solution, as shown in Figure 4. Click the &lt;code&gt;employeeList&lt;/code&gt; attribute's corresponding “blue planet" icon, select the &lt;strong&gt;Planning Value Range Provider &lt;/strong&gt;checkbox, and assign the &lt;strong&gt;Identifier&lt;/strong&gt; as &lt;code&gt;employeeRange&lt;/code&gt;. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Creating the data object DutyRoster as the planning solution in Decision Central" data-entity-type="file" data-entity-uuid="849c30ce-403f-482d-a128-54e25e42d331" src="https://developers.redhat.com/sites/default/files/inline-images/Diagram4_0.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 4: Create the data object &lt;code&gt;DutyRoster&lt;/code&gt; as the planning solution.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt;Reopen &lt;code&gt;shiftAssignment&lt;/code&gt;. Click the &lt;code&gt;employee&lt;/code&gt; attribute's corresponding blue planet icon, select the &lt;strong&gt;Planning variable&lt;/strong&gt; checkbox, and assign &lt;code&gt;employeeRange&lt;/code&gt; as the &lt;strong&gt;Value Range Id&lt;/strong&gt;. See Figure 5. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="The planning variables and the value range ID are selected." data-entity-type="file" data-entity-uuid="199211a1-aa69-4834-ad6e-47a3d016386c" src="https://developers.redhat.com/sites/default/files/inline-images/Diagram5.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 5: Select the planning values (planning variables) and the value range ID.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt;Define the third constraint: If the employee requested a shift off, they will not be assigned to the shift. Assigning a negative value to a hard score means that it must not be broken. See Figure 6. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Configuring DayOffRequests as hard constraints." data-entity-type="file" data-entity-uuid="24e4e491-ae73-405d-88e3-28c184a180cc" src="https://developers.redhat.com/sites/default/files/inline-images/Diagram6.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 6: Create and assign &lt;code&gt;DayOffRequests&lt;/code&gt; as hard constraints.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt;Define the fifth constraint (Figure 7): All employees are assigned the same number of shifts. Ideally, this soft constraint should not be broken. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Creating a guided rule that balances the employee shift number as soft constraints." data-entity-type="file" data-entity-uuid="a3e5ae8b-95ee-4781-a875-2e3ec51c2618" src="https://developers.redhat.com/sites/default/files/inline-images/Diagram7.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 7: Create and balance the employee shift number as soft constraints.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt;Add the Solver configuration on the Score Director Factory page shown in Figure 8. This configuration uses the default algorithm, construction heuristic, and late acceptance. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="A sample Duty Roster Solver configuration shown on the Score Director Factory page." data-entity-type="file" data-entity-uuid="53e9d546-ff9f-4891-b2a6-17ff95e6f686" src="https://developers.redhat.com/sites/default/files/inline-images/Diagram8.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 8: Add the Duty Roster Solver configuration on the Score Director Factory page.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Build&lt;/strong&gt;. Make sure the build is successful.&lt;/li&gt; &lt;li&gt;When the build is successful, click &lt;strong&gt;Deploy&lt;/strong&gt;. Make sure the deployment is successful.&lt;/li&gt; &lt;li&gt;Get the URL of the rule on the Kie Server.&lt;/li&gt; &lt;li&gt; &lt;p&gt;Navigate to the execution servers. Get the URL as &lt;code&gt;&lt;Copied URL&gt;&lt;/code&gt; (see Figure 9).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="A sample URL shown in the Kie Server." data-entity-type="file" data-entity-uuid="89c2cb63-446b-4d97-ba9c-6c7d00dca5b0" src="https://developers.redhat.com/sites/default/files/inline-images/Diagram9.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 9: Get the URL from the Kie Server.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt;Open &lt;a href="https://www.postman.com/"&gt;Postman&lt;/a&gt;. Open a tab with the URL as &lt;em&gt;&lt;Copied URL&gt;/solvers/&lt;Solver name&gt;&lt;/em&gt; and use the PUT method to specify and publish the body, as shown in Figure 10. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Using the PUT method to specify and publish the body." data-entity-type="file" data-entity-uuid="00b333cf-c8dc-4dab-9cb4-5288b34308df" src="https://developers.redhat.com/sites/default/files/inline-images/Diagram10.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 10: Open the URL and use the PUT method to specify and publish the body.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt;Open another tab with the URL as &lt;code&gt;&lt;Copied URL&gt;/solvers/&lt;Solver name&gt;/state/solving&lt;/code&gt; and use the POST method. This posts the &lt;code&gt;DutyRoster&lt;/code&gt; class in XML format—that is, the content of the planning solution (see Figure 11). &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Using the POST method to post the content of the planning solution." data-entity-type="file" data-entity-uuid="70249a60-baa5-45be-95e3-e45fe4be193a" src="https://developers.redhat.com/sites/default/files/inline-images/Diagram12.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 11: Open the URL and use the POST method to post the content of the planning solution.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt;Open another tab with the URL as &lt;code&gt;&lt;Copied URL&gt;/solvers/&lt;solver name&gt;/bestsolution&lt;/code&gt; and use the GET method, as shown in Figure 12. This returns the result from the planning engine. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Using the GET method to return the result from the planning engine." data-entity-type="file" data-entity-uuid="d0fe894d-786b-4bb7-b5eb-5071ecf9b619" src="https://developers.redhat.com/sites/default/files/inline-images/Diagram13.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 12: Open the URL and use the GET method to return the result from the planning engine.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;See the &lt;a href="https://github.com/likhia/DutyRosterProject"&gt;Duty Roster project on GitHub&lt;/a&gt; for the completed project and sample input files.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/09/how-implement-employee-rostering-red-hat-business-optimizer" title="How to implement employee rostering with Red Hat Business Optimizer"&gt;How to implement employee rostering with Red Hat Business Optimizer&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/yhdzn3y8ujE" height="1" width="1" alt=""/&gt;</summary><dc:creator>Li Khia Lim</dc:creator><dc:date>2021-06-09T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/09/how-implement-employee-rostering-red-hat-business-optimizer</feedburner:origLink></entry><entry><title type="html">Quarkus 1.13.7.Final released - Maintenance release</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/BkeVDRu9dxw/" /><author><name /></author><id>https://quarkus.io/blog/quarkus-1-13-7-final-released/</id><updated>2021-06-09T00:00:00Z</updated><content type="html">And here comes 1.13.7.Final, our (most probably!) last round of bugfixes for the 1.13 release. If you are not using 1.13 already, please refer to the 1.13 migration guide. Full changelog You can get the full changelog of 1.13.7.Final on GitHub. 2.0 coming soon We will soon start a new...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/BkeVDRu9dxw" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/quarkus-1-13-7-final-released/</feedburner:origLink></entry><entry><title type="html">Infinispan (Red Hat Data Grid) featured again in Red Hat Developers</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/3vbfEyfUcjM/infinispan-redhat-summit-quarkus" /><author><name>Katia Aresti</name></author><id>https://infinispan.org/blog/2021/06/08/infinispan-redhat-summit-quarkus</id><updated>2021-06-08T12:00:00Z</updated><content type="html">Dear Infinispan Community, The Infinispan team is pleased to share the follow up article published on the Red Hat Developer blog. Part two of a two-part series of articles, this blog post focuses on how the was used to create all the configuration and deployment descriptors for the online hybrid-cloud Battleship game that featured at this year’s Red Hat Summit Keynote. You can read the blog post The Infinispan Operator is available from the . Enjoy your reading!&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/3vbfEyfUcjM" height="1" width="1" alt=""/&gt;</content><dc:creator>Katia Aresti</dc:creator><feedburner:origLink>https://infinispan.org/blog/2021/06/08/infinispan-redhat-summit-quarkus</feedburner:origLink></entry><entry><title type="html">Kogito Task Management API</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/pYVrUNudfSI/kogito-task-management-api.html" /><author><name>Francisco Javier Tirado Sarti</name></author><id>https://blog.kie.org/2021/06/kogito-task-management-api.html</id><updated>2021-06-08T10:46:19Z</updated><content type="html">In a previous we discussed the task process API, which dealt with tasks from a process perspective. As explained in there, the task model depends on data defined by users. But, regardless of the task related information provided by users through BPMN processes, every existing task in Kogito has some predefined fields, mainly related to security policies, which are handled by Tasks Management API. Since Task Management API is a security-sensitive API, it is often referred as Admin API, but that name might cause confusion and should be avoided. Reason is that  this API is not intended for an administrator to change all task parameters arbitrarily, but for any user with enough privileges (in future, when Kogito defines a global authentication policy, management endpoint will be available only to users with certain privileges. Sdministrators will be of course included among them)  to modify those task parameters not exposed by Task Process API.  One important implication of this line of thought is that users with administrative privileges must use Task process API, as any other regular user, to perform phase transitions, change output parameters or add comments and attachments. TASK MANAGEMENT INFORMATION The information that can be consulted or modified by Task Management API consist of: * Description. Human readable description of the task     * Priority. A string field that indicates task priority.         * Potential Owners. The list of users and the list of groups which can claim ownership     of the task     * Administrators. The list of users and the list of groups which are task administrators, meaning that they can perform API process calls without being the current owners.         * Input parameters. A list of key value pairs. Although it includes the input model, which as you recall is automatically calculated during process execution, from a management perspective you will probably be more interested in interacting with additional task metadata information, also incorporated here. Fields like name, actorId or skippable, which might be useful to to change in certain specific scenarios, but cannot be modified using Task process API, because they are not part of the task model.     Some readers might be wondering, what about the current task owner? Following the rationale previously exposed, since the current task owner is expected to be established by using Claim o Release transitions, it was decided to not include it in the list of parameters that can be modified by Task Management API. Again, the administrative user can use the regular Task Process API to do so. TASK MANAGEMENT OPERATIONS Task management API consists of just one resource, which, in an outburst of originality, is called task. Endpoint URI match this template http://&lt;host&gt;:&lt;port&gt;/management/processes/&lt;process id&gt;/instances/&lt;process instance id&gt;/tasks/&lt;task Id&gt; Task management endpoint supports following HTTP methods:  * GET is used to consult task information. * PUT is used to replace whole task information, meaning you need to pass the whole task. Therefore, if you did not include a field in the request, this field will be set to null.   * PATCH is used to replace only certain fields, meaning that you pass only those fields you want to modify. Since usually you only need to modify a subset of all the fields, this will be the method you will be employing most.  RETRIEVING TASK INFORMATION We will illustrate management API usage by employing the example from previous post, adding task-management add-on to the project &lt;dependency&gt; &lt;groupId&gt;org.kie.kogito&lt;/groupId&gt; &lt;artifactId&gt;task-management-quarkus-addon&lt;/artifactId&gt; &lt;/dependency&gt; Since you already have the proper process instance Id and task Id, invoking GET http://localhost:8080/management/processes/approvals/instances/&lt;process instance id&gt;/tasks/&lt;task Id&gt; will return as response a JSON similar to this one { "description": null, "priority": null, "potentialUsers": [ "manager" ], "potentialGroups": [ "managers" ], "excludedUsers": [], "adminUsers": [], "adminGroups": [], "inputParams": { "TaskName": "firstLineApproval", "NodeName": "First Line Approval", "Skippable": "true", "ActorId": "manager", "traveller": { "firstName": "John", "lastName": "Doe", "email": "jon.doe@example.com", "nationality": "American", "address": { "street": "main street", "city": "Boston", "zipCode": "10005", "country": "US" } }, "GroupId": "managers" } } Note that inputParams field contains a traveller instance, which is the model defined for firstLineApproval task, but also includes NodeName, Skippable and ActorId, which are not part of it.   UPDATING TASK INFORMATION Let’s assume you want to add a description, change skippable input parameter to false and add user menganito to the list of potential users for that task instance, you should invoke  PATCH http://localhost:8080/management/processes/approvals/instances/&lt;process instance id&gt;/tasks/&lt;task Id&gt; providing as body { "inputParams" : {"Skippable":false}, "description" : " Real Betis Balompie is the best soccer team in the world", "potentialUsers":["manager","meganito"] } Please note that when using PATCH you only need to specify those fields that you want to modify, but if the field data type is a JSON collection, you need to specify all the items in the collection. That’s why we only provide skippable as inputParams value (because input parameters are merged) but the complete list for potentialUsers fields (because collections are not merged).  The response of previous invocation will consist on the whole resource properly modified { "description": " Real Betis Balompie is the best soccer team in the world", "priority": null, "potentialUsers": [ "manager", “menganito” ], "potentialGroups": [ "managers" ], "excludedUsers": [], "adminUsers": [], "adminGroups": [], "inputParams": { "TaskName": "firstLineApproval", "NodeName": "First Line Approval", "Skippable": false, "ActorId": "manager", "traveller": { "firstName": "John", "lastName": "Doe", "email": "jon.doe@example.com", "nationality": "American", "address": { "street": "main street", "city": "Boston", "zipCode": "10005", "country": "US" } }, "GroupId": "managers", } } CONCLUSION  In this post we discussed the Task Management API, a brief (just one resource) but powerful one, which is intended to interact with task information that is not part of the process model. Next post, the last of this series, will describe how to establish task deadlines, whose main purpose is to make sure that a task does not fall into oblivion.  The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/pYVrUNudfSI" height="1" width="1" alt=""/&gt;</content><dc:creator>Francisco Javier Tirado Sarti</dc:creator><feedburner:origLink>https://blog.kie.org/2021/06/kogito-task-management-api.html</feedburner:origLink></entry><entry><title>Create and manage Red Hat Data Grid services in the hybrid cloud</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/pMNZPcl6z5I/create-and-manage-red-hat-data-grid-services-hybrid-cloud" /><author><name>Katia Aresti, Don Naro, Ryan Emerson</name></author><id>91ad6083-efbd-47d4-bdac-6b89d01fd879</id><updated>2021-06-08T07:00:00Z</updated><published>2021-06-08T07:00:00Z</published><summary type="html">&lt;p&gt;In a &lt;a href="https://developers.redhat.com/articles/2021/05/28/building-real-time-leaderboard-red-hat-data-grid-and-quarkus-hybrid-kubernetes"&gt;recent article&lt;/a&gt;, we described how we used &lt;a href="https://developers.redhat.com/products/datagrid/overview"&gt;Red Hat Data Grid&lt;/a&gt;, built from the &lt;a href="https://infinispan.org/"&gt;Infinispan&lt;/a&gt; community project, to deliver a global leaderboard that tracked real-time scores for an online game.&lt;/p&gt; &lt;p&gt;In this article, we’re back to demonstrate how we used the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.1/html/running_data_grid_on_openshift/index"&gt;Red Hat Data Grid Operator&lt;/a&gt; on &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; to create and manage services running on &lt;a href="https://aws.amazon.com/"&gt;AWS&lt;/a&gt; (Amazon Web Services), &lt;a href="https://cloud.google.com/"&gt;GCP&lt;/a&gt; (Google Cloud Platform), and &lt;a href="https://azure.microsoft.com/"&gt;Microsoft Azure&lt;/a&gt;. In this case, we used the Data Grid Operator to create a global Data Grid cluster across multiple cloud platforms that appeared as a single Data Grid service to external consumers.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The global leaderboard was featured during Burr Sutter's Red Hat Summit keynote in April 2021. &lt;a href="https://developers.redhat.com/summit/2021"&gt;Get a program schedule&lt;/a&gt; for the second half of this year's virtual summit, coming June 15 to 16, 2021.&lt;/p&gt; &lt;h2&gt;Global leaderboard across the hybrid cloud&lt;/h2&gt; &lt;p&gt;Figure 1 shows the global leaderboard that we introduced in our &lt;a href="https://developers.redhat.com/articles/2021/05/28/building-real-time-leaderboard-red-hat-data-grid-and-quarkus-hybrid-kubernetes"&gt;previous article&lt;/a&gt;.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Cross-site replication with three sites." data-entity-type="file" data-entity-uuid="8b2c34bf-51c4-4db0-96f7-9a247cf40897" src="https://developers.redhat.com/sites/default/files/inline-images/cross-site-replication_0.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1: Cross-site replication using the Data Grid Operator.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;As we explained in the previous article, the online game and all related services were hosted on different cloud providers in three separate geographical regions. To ensure the system performed well and was responsive enough to show real-time results for each match, we used Data Grid to provide low-latency access to in-memory data stored close to the player’s physical location.&lt;/p&gt; &lt;p&gt;The leaderboard represents a global ranking of players, so the system needs to use all scores across each of the three clusters to determine overall winners. To achieve all this, we devised a solution that brought together the following technologies:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Indexing and querying caches using Protobuf encoding.&lt;/li&gt; &lt;li&gt;Quarkus extensions for Infinispan, RESTEasy, Websockets, and Scheduler.&lt;/li&gt; &lt;li&gt;Data Grid cross-site replication.&lt;/li&gt; &lt;li&gt;Data Grid Operator for OpenShift.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Data Grid Operator&lt;/h2&gt; &lt;p&gt;To make our lives easier, we created a &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_data_grid"&gt;Data Grid Operator&lt;/a&gt; subscription on each OpenShift cluster at each data center. The Operator then automatically configured and managed the Data Grid server nodes and did all the heavy lifting to establish cross-site network connections. As a result, we didn’t have as much deployment complexity to deal with, and could concentrate on our implementation. The following Infinispan custom resource contains the &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; API URL for each site and a reference to a secret that contains each cluster’s credentials. We referenced each site name in our cache’s &lt;code&gt;&lt;backups/&gt;&lt;/code&gt; configuration.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: infinispan.org/v1 kind: Infinispan metadata: name: datagrid spec: image: quay.io/infinispan/server:12.1.0.Final container: cpu: "1000m" memory: 4Gi expose: type: LoadBalancer replicas: 2 security: endpointEncryption: type: None endpointAuthentication: false service: type: DataGrid sites: local: name: SITE_NAME expose: type: LoadBalancer locations: - name: AWS url: openshift://api.summit-aws.28ts.p1.openshiftapps.com:6443 secretName: aws-token - name: GCP url: openshift://api.summit-gcp.eior.p2.openshiftapps.com:6443 secretName: gcp-token - name: AZURE url: openshift://api.g9dkpkud.centralus.aroapp.io:6443 secretName: azure-token logging: categories: org.jgroups.protocols.TCP: error org.jgroups.protocols.relay.RELAY2: error&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To create and initialize our caches, we used a combination of the Cache and Batch custom resources, which the Data Grid Operator provides. We'll look at those next.&lt;/p&gt; &lt;h2&gt;The Cache custom resource&lt;/h2&gt; &lt;p&gt;The caches that we needed to back up to other locations were &lt;code&gt;game&lt;/code&gt;, &lt;code&gt;players-scores&lt;/code&gt;, and &lt;code&gt;players-shots&lt;/code&gt;. We created these caches with the following Cache custom resource, where the &lt;code&gt;CACHE_NAME&lt;/code&gt; and &lt;code&gt;BACKUP_SITE_&lt;/code&gt; placeholders were replaced with correct values for each site deployment:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- apiVersion: infinispan.org/v2alpha1 kind: Cache metadata: name: CACHE_NAME spec: clusterName: datagrid name: CACHE_NAME adminAuth: secretName: cache-credentials template: | &lt;infinispan&gt; &lt;cache-container&gt; &lt;distributed-cache name="game" statistics="true"&gt; &lt;encoding&gt; &lt;key media-type="text/plain" /&gt; &lt;value media-type="text/plain" /&gt; &lt;/encoding&gt; &lt;backups&gt; &lt;backup site="BACKUP_SITE_1" strategy="ASYNC" enabled="true"&gt; &lt;take-offline min-wait="60000" after-failures="3" /&gt; &lt;/backup&gt; &lt;backup site="BACKUP_SITE_2" strategy="ASYNC" enabled="true"&gt; &lt;take-offline min-wait="60000" after-failures="3" /&gt; &lt;/backup&gt; &lt;/backups&gt; &lt;/distributed-cache&gt; &lt;/cache-container&gt; &lt;/infinispan&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;The Batch custom resource&lt;/h2&gt; &lt;p&gt;To create the other caches we needed for our system, and to upload all our &lt;code&gt;*.proto&lt;/code&gt; (Protobuf) schemas to our Data Grid cluster, we used the Batch custom resource. The Batch custom resource executes the &lt;code&gt;batch&lt;/code&gt; file in a &lt;code&gt;ConfigMap&lt;/code&gt; using the Data Grid command-line interface (CLI). We were able to take advantage of full CLI capabilities for manipulating caches without worrying too much about the authentication and connection details. Here’s the Batch custom resource we used to upload our schema, create the &lt;code&gt;players&lt;/code&gt; and &lt;code&gt;match-instances&lt;/code&gt; caches at each site, and then put an initial entry into the &lt;code&gt;game&lt;/code&gt; cache:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: infinispan.org/v2alpha1 kind: Batch metadata: name: datagrid spec: cluster: datagrid configMap: datagrid-batch --- apiVersion: v1 kind: ConfigMap metadata: name: datagrid-batch data: LeaderboardServiceContextInitializer.proto: | // File name: LeaderboardServiceContextInitializer.proto // Generated from : com.redhat.model.LeaderboardServiceContextInitializer syntax = "proto2"; package com.redhat; enum GameStatus { PLAYING = 1; WIN = 2; LOSS = 3; } enum ShipType { CARRIER = 1; SUBMARINE = 2; BATTLESHIP = 3; DESTROYER = 4; } /** * @Indexed */ message PlayerScore { /** * @Field(index=Index.YES, analyze = Analyze.NO, store = Store.YES) */ optional string userId = 1; optional string matchId = 2; /** * @Field(index=Index.YES, analyze = Analyze.NO, store = Store.YES) */ optional string gameId = 3; optional string username = 4; /** * @Field(index=Index.YES, analyze = Analyze.NO, store = Store.YES) */ optional bool human = 5; /** * @Field(index=Index.YES, analyze = Analyze.NO, store = Store.YES) * @SortableField */ optional int32 score = 6; /** * @Field(index=Index.YES, analyze = Analyze.NO, store = Store.YES) * @SortableField */ optional int64 timestamp = 7; /** * @Field(index=Index.YES, analyze = Analyze.NO, store = Store.YES) */ optional GameStatus gameStatus = 8; /** * @Field(index=Index.YES, analyze = Analyze.NO, store = Store.YES) */ optional int32 bonus = 9; } /** * @Indexed */ message Shot { /** * @Field(index=Index.YES, analyze = Analyze.NO, store = Store.YES) */ required string userId = 1; required string matchId = 2; /** * @Field(index=Index.YES, analyze = Analyze.NO, store = Store.YES) */ optional string gameId = 3; /** * @Field(index=Index.YES, analyze = Analyze.NO, store = Store.YES) */ optional bool human = 4; /** * @Field(index=Index.YES, analyze = Analyze.NO, store = Store.YES) */ optional int64 timestamp = 5; /** * @Field(index=Index.YES, analyze = Analyze.NO, store = Store.YES) */ optional ShotType shotType = 6; /** * @Field(index=Index.YES, analyze = Analyze.NO, store = Store.YES) */ optional ShipType shipType = 7; } enum ShotType { HIT = 1; MISS = 2; SUNK = 3; } batch: | schema --upload=/etc/batch/LeaderboardServiceContextInitializer.proto LeaderboardServiceContextInitializer.proto create cache --file=/etc/batch/match-instances.xml match-instances create cache --file=/etc/batch/players.xml players cd caches/game put --encoding=application/json --file=/etc/batch/game-config.json game game-config.json: "{\n\t\"id\": \"uuidv4\",\n\t\"state\": \"lobby\"\n}\n" game.xml: | &lt;infinispan&gt; &lt;cache-container&gt; &lt;distributed-cache name="game" statistics="true"&gt; &lt;encoding&gt; &lt;key media-type="text/plain" /&gt; &lt;value media-type="text/plain" /&gt; &lt;/encoding&gt; &lt;/distributed-cache&gt; &lt;/cache-container&gt; &lt;/infinispan&gt; match-instances.xml: | &lt;infinispan&gt; &lt;cache-container&gt; &lt;distributed-cache name="players" statistics="true"&gt; &lt;encoding&gt; &lt;key media-type="text/plain" /&gt; &lt;value media-type="text/plain" /&gt; &lt;/encoding&gt; &lt;/distributed-cache&gt; &lt;/cache-container&gt; &lt;/infinispan&gt; players.xml: | &lt;infinispan&gt; &lt;cache-container&gt; &lt;distributed-cache name="players" statistics="true"&gt; &lt;encoding&gt; &lt;key media-type="text/plain" /&gt; &lt;value media-type="text/plain" /&gt; &lt;/encoding&gt; &lt;/distributed-cache&gt; &lt;/cache-container&gt; &lt;/infinispan&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, we’ve shown you how we created a system built with Red Hat Data Grid and &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt; with &lt;a href="https://quarkus.io/guides/rest-json"&gt;RESTEasy&lt;/a&gt;, &lt;a href="https://quarkus.io/guides/websockets"&gt;Websockets&lt;/a&gt;, &lt;a href="https://quarkus.io/guides/scheduler"&gt;Scheduler&lt;/a&gt;, and the &lt;a href="https://quarkus.io/guides/infinispan-client"&gt;infinispan-client&lt;/a&gt; extensions, along with &lt;a href="https://developers.google.com/protocol-buffers"&gt;Protobuf&lt;/a&gt; encoding. The resulting system creates a global ranking of game players across three different cloud providers in separate geographic regions. We hope all of these details will inspire you to start using Data Grid, or the Infinispan project, for other hybrid cloud use cases. If you’re interested in finding out more, please feel free to start with the &lt;a href="https://access.redhat.com/products/red-hat-data-grid"&gt;Data Grid project page&lt;/a&gt; or visit our &lt;a href="https://infinispan.org/"&gt;Infinispan community website&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/08/create-and-manage-red-hat-data-grid-services-hybrid-cloud" title="Create and manage Red Hat Data Grid services in the hybrid cloud"&gt;Create and manage Red Hat Data Grid services in the hybrid cloud&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/pMNZPcl6z5I" height="1" width="1" alt=""/&gt;</summary><dc:creator>Katia Aresti, Don Naro, Ryan Emerson</dc:creator><dc:date>2021-06-08T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/08/create-and-manage-red-hat-data-grid-services-hybrid-cloud</feedburner:origLink></entry><entry><title>How to size your projects for Red Hat's single sign-on technology</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/8lHuPmBeSk4/how-size-your-projects-red-hats-single-sign-technology" /><author><name>Nicolas Massé</name></author><id>8de11e09-556e-4809-b65c-ef2af721ed32</id><updated>2021-06-07T07:00:00Z</updated><published>2021-06-07T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;Red Hat's single sign-on (SSO) technology&lt;/a&gt; is an identity and access management tool included in the Red Hat Middleware Core Services Collection that's based on the well-known &lt;a href="https://www.keycloak.org/"&gt;Keycloak&lt;/a&gt; open source project. As with other Red Hat products, users have to acquire subscriptions, which are priced according to the number of cores or vCPU used to deploy the product.&lt;/p&gt; &lt;p&gt;This presents an interesting problem for pre-sales engineers like me. To help my customers acquire the correct number of subscriptions, I need to sketch the target architecture and count how many cores they need. This would not be a problem if off-the-shelf performance benchmarks were available; however, they are not.&lt;/p&gt; &lt;p&gt;This article will help colleagues and customers estimate their SSO projects more precisely. We will examine the performance benchmarks I ran, how I designed them, the results I gathered, and how I drew conclusions to size my SSO project.&lt;/p&gt; &lt;h2&gt;Performance benchmarks: Framing the problem&lt;/h2&gt; &lt;p&gt;Performance benchmarking is a broad topic, and if you don't correctly frame the problem, it is easy to answer a question that has not been asked. So, for my situation, I wanted to answer the following questions:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Which architectural choices have the most impact on the number of cores used?&lt;/li&gt; &lt;li&gt;Which part of the user’s session has the most impact on the number of cores used? Opening the session? Renewing the token? Validating the token?&lt;/li&gt; &lt;li&gt;How many transactions per second (TPS) can we expect per core from typical server-grade hardware?&lt;/li&gt; &lt;/ul&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Between a low-end server CPU and a high-end server CPU, there can be a significant gap in terms of single-thread performance. Therefore, I am not interested in precise figures, but rather the order of magnitude (e.g., 1, 10, 100, or 1,000 TPS?).&lt;/p&gt; &lt;h2&gt;Planning the performance assessment&lt;/h2&gt; &lt;p&gt;In the Keycloak repository, there is a &lt;a href="https://github.com/keycloak/keycloak/tree/master/testsuite/performance"&gt;test suite&lt;/a&gt; that assesses Keycloak performance. After careful study, I decided not to use it for two reasons:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;It does not answer the questions I listed in the previous section.&lt;/li&gt; &lt;li&gt;The test suite is tightly coupled with the Keycloak development environment, so it would be difficult to reuse on customer sites if needed.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;I divided my approach into four main steps:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Setting up SSO and the underlying services (database, load balancer, etc.).&lt;/li&gt; &lt;li&gt;Filling the SSO database with users, clients, and realms.&lt;/li&gt; &lt;li&gt;Generating load on the SSO server.&lt;/li&gt; &lt;li&gt;Collecting performance data.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Every customer is different, and sometimes various tools and techniques might be required on customer sites. With that in mind, I designed those four steps to be loosely coupled, so you can adjust each step to use a different tool or technique.&lt;/p&gt; &lt;p&gt;Whenever possible, I reuse the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.4/html/server_administration_guide/export_import"&gt;Keycloak Realm file&lt;/a&gt; as a pivot format. It is used by the script that loads the database and by the load testing tool that generates load on the SSO server.&lt;/p&gt; &lt;p&gt;To set up SSO and the underlying services, I chose to use &lt;a href="https://github.com/nmasse-itix/keycloak-loadtesting-setup/tree/33168b763c3f27486d0e883b7b008783835f3b22"&gt;Ansible playbooks&lt;/a&gt; that deploy components as Podman containers. They are easy for new team members to use and understand; plus, they are widely used on customer sites.&lt;/p&gt; &lt;p&gt;I created a dedicated tool named &lt;a href="https://github.com/nmasse-itix/keycloak-import-realm/tree/v0.1.4"&gt;kci&lt;/a&gt; to load the database with users, clients, and realms.&lt;/p&gt; &lt;p&gt;To generate the load on the SSO server, I used &lt;a href="https://github.com/nmasse-itix/keycloak-loadtesting-k6/tree/2475c668116b2591fd9c88fb6caa5c9d77c66e18"&gt;K6&lt;/a&gt;, a novel performance testing tool written in Go that uses plain JavaScript for the test definition. (Have a look at &lt;a href="https://k6.io/"&gt;k6.io&lt;/a&gt; if you aren't familiar with it.)&lt;/p&gt; &lt;p&gt;The test results are collected by Prometheus and presented through Grafana, as shown in Figure 1. For a primer on K6, Prometheus, and Grafana, I recommend reading &lt;a href="https://www.itix.fr/blog/how-to-run-performance-tests-with-k6-prometheus-grafana/"&gt;this article.&lt;/a&gt;&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/04/k6-grafana-prometheus.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/04/k6-grafana-prometheus.png?itok=sGFLqEvy" width="600" height="321" alt="K6, Grafana and Prometheus conducting a benchmark on Red Hat SSO." title="k6, grafana &amp;amp; prometheus" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; K6, Grafana and Prometheus conducting a benchmark on Red Hat SSO. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: K6, Grafana, and Prometheus conducting a benchmark on SSO.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Designing the benchmark scenarios&lt;/h2&gt; &lt;p&gt;Scenarios are a key part of the performance benchmark. Carefully chosen scenarios will help to answer the questions we established earlier. One should devise scenarios as scientific experiments:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Choose a control that sets a baseline and ensures that performance remains constant at any point in time.&lt;/li&gt; &lt;li&gt;Craft experiments by changing one (and only one) parameter of the control experiment at a time. The experiment will reflect this parameter's effect on performance.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;For my control experiment, I chose the following configuration as the baseline:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Two SSO servers, each one having a dedicated single physical core.&lt;/li&gt; &lt;li&gt;The SSO servers are backed by a PostgreSQL instance.&lt;/li&gt; &lt;li&gt;A Traefik reverse proxy is set in front of the SSO servers to spread the load.&lt;/li&gt; &lt;li&gt;The database is loaded with 5,000 users and 500 clients spread amongst 5 realms.&lt;/li&gt; &lt;li&gt;No specific performance tuning is applied to any of those components.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;And then from this baseline, I devised the following scenarios:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Offline tokens:&lt;/strong&gt; Same as baseline, but offline tokens are requested instead of regular tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MariaDB:&lt;/strong&gt; Same as baseline, but with MariaDB instead of PostgreSQL.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;One node:&lt;/strong&gt; Same as baseline, but with only one SSO instance having two physical cores.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Size S:&lt;/strong&gt; Same as baseline, but with less data in the database (100 users and 10 clients in 1 realm).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Size L:&lt;/strong&gt; Same as baseline, but with more data in the database (100,000 users and 10,000 clients spread in 10 realms).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PBKDF2 with 1 iteration:&lt;/strong&gt; Same as baseline, but with the PBKDF2 configured with 1 iteration instead of 27,500.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LDAP:&lt;/strong&gt; Same as baseline, but with users loaded in an OpenLDAP instance instead of the SSO database.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In the baseline and the scenarios just described, I chose to collect the following metrics:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The user opens its SSO session: How many TPS?&lt;/li&gt; &lt;li&gt;The user refreshes its access token: How many TPS?&lt;/li&gt; &lt;li&gt;The user token is introspected using the tokeninfo endpoint: How many TPS?&lt;/li&gt; &lt;li&gt;The user token is introspected using the userinfo endpoint: How many TPS?&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;I chose to focus only on the number of transactions per second because it is an objective measure (the maximum you can get). Latency figures are sometimes discussed on customer sites, but there is not much we can do about it. Latency refers to the minimum number of CPU cycles required to serve the request, and it can only increase when bottlenecks (such as CPU contention) start to appear. Said differently: There is a typical latency, and that latency starts to skyrocket when a tipping point is passed. As long as you do not cross that point, there is nothing interesting happening.&lt;/p&gt; &lt;p&gt;I ran the performance benchmarks on a bare-metal server: an HP MicroServer gen8, with a &lt;a href="https://www.cpubenchmark.net/cpu.php?cpu=Intel+Xeon+E3-1240+V2+%40+3.40GHz&amp;id=1190"&gt;Xeon E3-1240v2 CPU&lt;/a&gt; and 16GB of RAM. Only two physical cores of the Xeon CPU are dedicated to SSO servers. The rest has been allocated to the load balancer, the database, and the operating system.&lt;/p&gt; &lt;h2&gt;Note on the PBKDF2 function&lt;/h2&gt; &lt;p&gt;In the next section, you will see a big increase in the throughput depending on where the user passwords are stored. Let's take a closer look at the Password-Based Key Derivation Function 2 (PBKDF2) function.&lt;/p&gt; &lt;p&gt;By default, Red Hat's single sign-on tool stores the user passwords in its internal database and hashes those passwords using the PBKDF2 function. The purpose of this function is to be CPU intensive to slow down brute force attacks to a point where they become too expensive or too long to be practical. One can adjust the strength of this protection by configuring the number of iterations.&lt;/p&gt; &lt;p&gt;SSO performs 27,500 PBKDF2 iterations by default. &lt;a href="https://en.wikipedia.org/wiki/PBKDF2"&gt;Wikipedia&lt;/a&gt; tells us more about what is a safe choice for the number of iterations.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: When the standard was written in the year 2000, the recommended minimum number of iterations was 1,000, but the parameter was designed to increase over time to align with CPU speeds. A Kerberos standard in 2005 recommended 4,096 iterations; Apple reportedly used 2,000 for iOS 3, and 10,000 for iOS 4. In 2011, LastPass used 5,000 iterations for JavaScript clients and 100,000 iterations for server-side hashing.&lt;/p&gt; &lt;p&gt;This means you cannot store passwords in a secure way and at the same time exhibit a high number of TPS per physical core during the user session openings. By definition.&lt;/p&gt; &lt;p&gt;However, you can configure SSO to use passwords from another repository (your Active Directory, OpenLDAP, or Red Hat Directory Server, for instance) and rely on the security mechanisms of those repositories. That would be a way to have the best of both worlds.&lt;/p&gt; &lt;h2&gt;Results&lt;/h2&gt; &lt;p&gt;Based on the results, I was able to draw the following conclusions:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The key dimensions of an SSO project are the number of user session openings per second and where the user passwords are stored. &lt;ul&gt;&lt;li&gt;SSO can sustain around 75 TPS per physical core if the user passwords are stored in a third-party system (an LDAP directory, for instance) or if the PBKDF2 function is configured with one iteration.&lt;/li&gt; &lt;li&gt;Otherwise, SSO sustains slightly less than 10 TPS per physical core.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Refreshing tokens is less costly: SSO can sustain around 200 TPS per physical core.&lt;/li&gt; &lt;li&gt;Introspecting a token is pretty cheap. SSO can sustain around 1,400–1,700 TPS per physical core; 1,400 TPS using the tokeninfo endpoint, and 1,700 TPS using the userinfo endpoint.&lt;/li&gt; &lt;li&gt;The choice of the database has no significant impact on performance.&lt;/li&gt; &lt;li&gt;Using offline tokens instead of regular tokens has a slight impact on performance (a 10% penalty).&lt;/li&gt; &lt;li&gt;When high availability is not required, the one node setup shows a 20% increase in the number of TPS per physical core.&lt;/li&gt; &lt;/ul&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: One physical core = two threads = two vCPU.&lt;/p&gt; &lt;p&gt;See &lt;a href="https://github.com/nmasse-itix/keycloak-loadtesting/tree/35d4284be5a03a089c21f7b7baa6611b04edf7f7"&gt;the repository containing the complete result set.&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The SSO performance benchmarks presented in this article are by no means a definitive answer to this topic. It should instead be considered as an initial work to help the community, Red Hatters, and our customers better size their single sign-on projects.&lt;/p&gt; &lt;p&gt;More work is required to test other hypotheses, such as the impact of an external &lt;a href="https://developers.redhat.com/products/datagrid/overview"&gt;Red Hat Data Grid&lt;/a&gt; server, possible optimizations here and there, the possibility of achieving linear scalability with a high number of nodes, or even the impact of deployments within &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/07/how-size-your-projects-red-hats-single-sign-technology" title="How to size your projects for Red Hat's single sign-on technology"&gt;How to size your projects for Red Hat's single sign-on technology&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/8lHuPmBeSk4" height="1" width="1" alt=""/&gt;</summary><dc:creator>Nicolas Massé</dc:creator><dc:date>2021-06-07T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/07/how-size-your-projects-red-hats-single-sign-technology</feedburner:origLink></entry></feed>
